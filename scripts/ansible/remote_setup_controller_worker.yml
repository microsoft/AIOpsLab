---
# Control Node Setup
- hosts: control_node
  become: true
  vars:
    # user_home_base: Set in inventory. Change based on environment. E.g., Use "/home" for cloud VMs, "/users" for Emulab
    user_home: "{{ user_home_base }}/{{ k8s_user }}"
    kubeconfig_path: "{{ user_home }}/.kube/config"
  tasks:
    # For Emulab: resolve ansible_host hostname to IP
    # For cloud VMs (Terraform): use private_ip for internal, ansible_host (public) for external
    - name: Determine control plane IPs
      set_fact:
        control_plane_ip: "{{ hostvars['control_node'].private_ip | default('') }}"
        control_plane_public_ip: "{{ hostvars['control_node'].ansible_host }}"

    - name: Resolve hostname if private_ip not set (Emulab)
      command: getent ahosts "{{ hostvars['control_node'].ansible_host }}"
      register: resolved_ip_output
      when: control_plane_ip == ''

    - name: Use resolved IP for Emulab
      set_fact:
        control_plane_ip: "{{ resolved_ip_output.stdout_lines[0].split(' ')[0] }}"
      when: control_plane_ip == ''

    - name: Display control plane IPs being used
      debug:
        msg:
          - "Control plane private IP: {{ control_plane_ip }}"
          - "Control plane public IP: {{ control_plane_public_ip }}"

    - name: Set control_plane_ip globally for workers
      add_host:
        name: "global"
        control_plane_ip: "{{ control_plane_ip }}"
        control_plane_public_ip: "{{ control_plane_public_ip }}"

    - name: Initialize Kubernetes control plane
      shell: |
        kubeadm init \
          --pod-network-cidr=10.244.0.0/16 \
          --cri-socket unix:///var/run/cri-dockerd.sock \
          --apiserver-advertise-address={{ control_plane_ip }} \
          --apiserver-cert-extra-sans={{ control_plane_public_ip }},{{ control_plane_ip }}
      args:
        creates: /etc/kubernetes/admin.conf
    - name: Ensure .kube directory exists
      file:
        path: "{{ user_home }}/.kube"
        state: directory
        mode: '0755'
        owner: "{{ k8s_user }}"
        group: "{{ k8s_user }}"
      become: true
    - name: Temporarily set permissions to read admin.conf
      file:
        path: /etc/kubernetes/admin.conf
        mode: '0644'
      become: true
    
    - name: Set up kube config for kubectl on control plane
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ kubeconfig_path }}"
        mode: '0600'
        remote_src: true
        owner: "{{ k8s_user }}"
        group: "{{ k8s_user }}"
      become: true

    - name: Restore admin.conf permissions
      file:
        path: /etc/kubernetes/admin.conf
        mode: '0600'
      become: true

    - name: Fetch admin.conf to localhost for kubeconfig (Mode B - remote access)
      fetch:
        src: /etc/kubernetes/admin.conf
        dest: "{{ lookup('env', 'HOME') }}/.kube/config"
        flat: yes
      become: true

    - name: Set local kubeconfig permissions
      delegate_to: localhost
      become: false
      file:
        path: "{{ lookup('env', 'HOME') }}/.kube/config"
        mode: '0600'

    - name: Update kubeconfig to use public IP for remote access
      delegate_to: localhost
      become: false
      replace:
        path: "{{ lookup('env', 'HOME') }}/.kube/config"
        regexp: 'server: https://[^:]+:6443'
        replace: 'server: https://{{ control_plane_public_ip }}:6443'
    - name: Generate kubeadm join command
      shell: kubeadm token create --print-join-command
      register: kubeadm_join_command
    - name: Extract kube_token and cert_hash from join command
      set_fact:
        kube_token: "{{ (kubeadm_join_command.stdout | regex_search('--token\\s+([\\w.]+)', '\\1')).0 }}"
        cert_hash: "{{ (kubeadm_join_command.stdout | regex_search('--discovery-token-ca-cert-hash\\s+sha256:([\\w]+)', '\\1')).0 }}"
  
    - name: Display kube_token
      debug:
        msg: "kube_token is {{ kube_token }}"
      no_log: true
    - name: Display cert_hash
      debug:
        msg: "cert_hash is {{ cert_hash }}"
      no_log: true
    - name: Install Flannel network plugin
      shell: |
        kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: flannel_result
      changed_when: "'created' in flannel_result.stdout or 'configured' in flannel_result.stdout"

    - name: Wait for Flannel pods to be ready
      shell: |
        kubectl get pods -n kube-flannel -l app=flannel --no-headers 2>/dev/null | grep -v Running || true
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: flannel_pods
      until: flannel_pods.stdout == ""
      retries: 30
      delay: 10
      changed_when: false

    - name: Untaint the control plane to allow pod scheduling
      shell: kubectl taint nodes --all node-role.kubernetes.io/control-plane- || true
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: untaint_result
      changed_when: "'untainted' in untaint_result.stdout"
# Worker Node Setup
- hosts: worker_nodes
  become: true
  vars:
    user_home: "{{ user_home_base }}/{{ k8s_user }}"
  tasks:
    - name: Check if worker already joined cluster
      stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Join Kubernetes cluster
      shell: |
        kubeadm join {{ hostvars['global'].control_plane_ip }}:6443 \
          --token {{ hostvars['control_node'].kube_token }} \
          --discovery-token-ca-cert-hash sha256:{{ hostvars['control_node'].cert_hash }} \
          --cri-socket unix:///var/run/cri-dockerd.sock
      when: not kubelet_conf.stat.exists
      register: join_result

    - name: Display join result
      debug:
        msg: "{{ join_result.stdout_lines | default(['Already joined']) }}"

# Final Verification
- hosts: control_node
  become: true
  vars:
    user_home: "{{ user_home_base }}/{{ k8s_user }}"
    kubeconfig_path: "{{ user_home }}/.kube/config"
  tasks:
    - name: Wait for all nodes to be Ready
      shell: |
        kubectl get nodes --no-headers | grep -v " Ready " || true
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: nodes_not_ready
      until: nodes_not_ready.stdout == ""
      retries: 30
      delay: 10
      changed_when: false

    - name: Display final cluster status
      shell: kubectl get nodes -o wide
      environment:
        KUBECONFIG: "{{ kubeconfig_path }}"
      register: cluster_status
      changed_when: false

    - name: Cluster setup complete
      debug:
        msg:
          - "============================================"
          - "Kubernetes Cluster Setup Complete!"
          - "============================================"
          - ""
          - "{{ cluster_status.stdout_lines }}"
          - ""
          - "Kubeconfig has been copied to your local machine."
          - "Run 'kubectl get nodes' to verify access."