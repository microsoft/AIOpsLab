# Echo
## verl_inference
The inferencer which generates rollouts, then sends rollouts to the server.
### Main code
/verl_inference/verl/trainer/ppo/ray_trainer.py
### Start the bash
```
./verl_inference/generate_rollouts_llama.sh
```

## verl_trainer
The trainer which uses the rollouts sended by inferencer to train the model.
### Main code
/verl_trainer/verl/trainer/ppo/ray_trainer.py
### Start the bash
```
./verl_trainer/train_llama.sh
```

## server
Send the rollouts generated by inferencer to the trainer; Synchronize the model weights of the trainer to the inferencer.
```
python server.py
```

## Local RL Workflow Harness

For quick end-to-end checks with the mock callback server and the public chat
completion endpoint:

1. Ensure the required dependencies are installed (`uvicorn`, `requests`).
2. Run the harness script which launches `service_api`, the mock Echo server,
   and the `mock_echo_agent` driver:
   ```
   python scripts/run_mock_echo_harness.py
   ```
   Override ports or chat endpoint if needed (`--help` for flags).
3. Inspect the saved callback payload at `logs/echo_runs_<timestamp>.json` or
   query the server directly:
   ```
   curl "http://127.0.0.1:8098/callbacks/runs?limit=5"
   ```
